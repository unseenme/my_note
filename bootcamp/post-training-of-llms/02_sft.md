# Supervised Fine-Tuning (SFT) for Language Models

## Overview of SFT

Supervised Fine-Tuning (SFT) is a core method for transforming general-purpose Language Models (LLMs) into task-specific assistants. The process involves training the model on pairs of user prompts and ideal assistant responses.

### SFT Core Mechanism

SFT teaches the base model (which primarily predicts the next token based on the prompt) to generate the desired answer by:

1.  **Input:** Providing a **Base Model** that, when unadjusted, might give generic or repetitive responses (e.g., answering a question with a counter-question).
2.  **Data:** Using a **Labeled Dataset** of paired user prompts and ideal assistant responses (e.g., "Tell me who you areâ€”I am Llama...").
3.  **Training:** Conducting SFT training by minimizing the **Cross-Entropy Loss** of the response, which maximizes the probability of generating the target response given the prompt:

    $$\mathcal{L}_{\text{SFT}} = -\sum_{i=1}^N \log \bigl(p_\theta(\text{Response}(i)\mid \text{Prompt}(i))\bigr)$$
    This loss encourages the model to "imitate" the labels, punishing any deviation from the labeled response.
4.  **Output:** Resulting in a **Fine-Tuned Model** that can provide appropriate, assistant-like replies to new queries (e.g., greeting the user instead of simply repeating the question).

## Optimal Use Cases for SFT

SFT serves as an essential bridge between pre-training and more advanced alignment methods. It is particularly effective for:

* **Eliciting New Model Behaviors:**
    * Converting a pre-trained model into an instruction-following assistant.
    * Enabling a model lacking inherent reasoning capabilities to learn basic reasoning.
    * Teaching a model to use specific tools without explicit instructions.
* **Improving Model Capabilities (Knowledge Distillation):** Training a smaller model to "distill" the capabilities of a more powerful large model by training on high-quality synthetic data generated by the larger one.

SFT is often the right choice when a model needs to quickly adapt to new behaviors and example data is available.

## Principles of SFT Data Curation

The effectiveness of SFT is critically dependent on **data quality**. The guiding principle is **"Quality over Quantity"**. A smaller, high-quality, and diverse set of samples is often more effective than a massive, inconsistent dataset, as SFT forces the model to imitate everything it sees, including poor answers.

Key data curation strategies include:

* **Distillation:** Using a stronger instruction model to generate responses and training a smaller model to imitate them, thereby transferring the strong model's capabilities.
* **Best-of-K / Rejection Sampling:** Generating multiple candidate responses for a single prompt and using a reward function to select the best one for the training data.
* **Filtering:** Selecting high-quality responses and prompts with good diversity from a larger SFT dataset to create a smaller, high-quality dataset.

## Full Fine-Tuning vs. Parameter-Efficient Fine-Tuning (PEFT)

When performing SFT (or any alignment method), a decision must be made on how to update the model weights:

| Method | Description | Pros | Cons |
| :--- | :--- | :--- | :--- |
| **Full Fine-Tuning** | A complete weight update matrix ($\Delta W$) is added to every layer, modifying all parameters. | Significant performance boost. | Requires substantial memory and computational resources. |
| **Parameter-Efficient Fine-Tuning (PEFT)** | E.g., LoRA introduces small, low-rank matrices (A and B) per layer to adjust parameters. | Reduces the number of trainable parameters and saves VRAM. | More limited learning and forgetting capabilities due to fewer updated parameters.

The choice between these strategies is a trade-off between performance requirements and resource constraints, with PEFT being highly popular in resource-limited environments.

---

**Summary:** SFT is a foundational alignment method that teaches a model to imitate desired behaviors by minimizing the negative log-likelihood of the target response. It's crucial for initiating new model behaviors and for knowledge distillation. Data quality is paramount, and the choice between full and efficient fine-tuning hinges on available resources and performance goals.