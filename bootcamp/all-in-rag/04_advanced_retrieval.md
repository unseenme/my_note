# Advanced Retrieval Techniques

# Section 1: Hybrid Search

**Hybrid Search** combines the strengths of **Sparse Vectors** and **Dense Vectors** to provide more accurate and robust results by utilizing both **keyword precision** and **semantic understanding**.

## I. Sparse Vectors vs. Dense Vectors

### 1.1 Sparse Vectors
Also known as "lexical vectors," these are a mathematical representation of traditional information retrieval methods based on word frequency statistics.
* They are typically **high-dimensional** (similar to vocabulary size) but mostly zero.
* They use a precise **"Bag-of-Words"** matching model, where non-zero values represent a word's importance (weight).
* **BM25** is a typical and widely used sparse vector scoring algorithm, whose core formula is:
    $$ Score(Q, D) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})} $$
* **Pros**: Highly interpretable, no training needed, good for precise keyword matching and specialized terms.
* **Cons**: Cannot understand semantics (e.g., synonyms), suffering from the **"lexical gap"**.

### 1.2 Dense Vectors
Also known as "semantic vectors," these are low-dimensional, dense floating-point representations learned through deep learning models.
* They map data into a continuous, meaningful **"semantic space"** to capture context and meaning.
* The distance and direction between vectors represent the relationship between the concepts they represent (e.g., gender, royalty).
* Typical examples include Word2Vec, GloVe, and embeddings generated by Transformer-based models (like BERT, GPT).
* **Pros**: Strong ability to understand synonyms, antonyms, and context; strong generalization ability.
* **Cons**: Poor interpretability (dimensions lack specific physical meaning), requires large amounts of data and compute for training, and challenging for Out-of-Vocabulary (OOV) words.

### 1.3 Example Comparison
* **Sparse Vectors** primarily store non-zero values, capturing keyword information but failing to recognize semantic similarity between different words.
* **Dense Vectors** use an array of values where every dimension has a meaning. The resulting vector's position in the semantic space allows it to match concepts even if no exact keywords are present in the query.

## II. Hybrid Retrieval
Hybrid Retrieval combines multiple search algorithms (most commonly sparse and dense retrieval) to enhance search result relevance and recall.
* **Main Goal**: To address the limitations of single retrieval techniques, leveraging the **precision** of sparse vectors and the **generalization** of dense vectors.

### 2.1 Technical Principles and Fusion Methods
Hybrid Retrieval typically executes both algorithms in parallel and then merges the two heterogeneous result sets into a unified ranking list.

#### 2.1.1 Reciprocal Rank Fusion (RRF)
* RRF focuses solely on the **rank** of each document in the respective result sets, not the original scores.
* The principle is: the higher a document ranks in different systems, the higher its final score.
* The scoring formula is:
    $$ RRF_{score}(d) = \sum_{i=1}^{k} \frac{1}{rank_i(d) + c} $$

#### 2.1.2 Weighted Linear Combination
* This method requires normalizing the scores from different retrieval systems (e.g., to the 0-1 range).
* A weight parameter $\alpha$ is then used for a linear combination:
    $$ Hybrid_{score} = \alpha \cdot Dense_{score} + (1 - \alpha) \cdot Sparse_{score} $$
* Adjusting $\alpha$ controls the influence of semantic similarity versus keyword matching in the final ranking.

---

# Section 2: Query Construction
**Query Construction** uses a Large Language Model (LLM) to "translate" a user's natural language query into a structured query language or a request with filtering conditions for a specific data source.

## I. Text-to-Metadata Filter
Metadata (like source, date, author) is often attached to document chunks, allowing for precise filtering alongside semantic search.
* The **Self-Query Retriever** is a core component for this function.
* **Workflow**:
    1.  Define the **metadata structure** and its meaning to the LLM.
    2.  **Query Parsing**: The LLM breaks the query into a **Query String** (for semantic search) and a **Metadata Filter** (structured filtering condition).
    3.  **Execute Query**: The retriever executes a combined query using both the search string and the metadata filter on the vector database.

## II. Text-to-Cypher
This technique extends query construction to complex data structures like **graph databases**.

### 2.1 What is Cypher?
Cypher is the most common query language for graph databases (like Neo4j), similar to SQL for relational databases. It uses an intuitive approach to match patterns and relationships in a graph.

### 2.2 Constructing the Knowledge Graph
A Knowledge Graph can be constructed by using an LLM to identify the main entities (nodes) and their relationships (edges) from unstructured text, and then storing the extracted triples (Subject-Predicate-Object) in the graph database.

### 2.3 Text-to-Cypher Workflow
The LLM converts the natural language question into a Cypher query by leveraging the graph database's schema information. The system feeds the user's question, the Knowledge Graph's schema (node labels, relationship types, and properties), and few-shot examples to the LLM.

---

# Section 3: Text-to-SQL
**Text-to-SQL** utilizes an LLM to translate a user's natural language question directly into an executable SQL query for relational databases.

## I. Business Challenges
1.  **Hallucination**: LLMs may "imagine" non-existent tables or fields, leading to invalid SQL.
2.  **Lack of Database Structure Understanding**: LLMs must correctly understand table structure, field meanings, and inter-table relationships to generate correct `JOIN` and `WHERE` clauses.
3.  **Handling Input Ambiguity**: The model needs tolerance and reasoning ability for vague or informal user expressions (e.g., "Who was last month's sales champion?").

## II. Optimization Strategies
1.  **Provide Accurate Database Schema**: Providing the LLM with the `CREATE TABLE` statements for relevant tables is fundamental for understanding structure, columns, data types, and foreign keys.
2.  **Few-shot Examples**: Including high-quality "Question-SQL" example pairs in the prompt significantly boosts the LLM's query generation accuracy.
3.  **RAG-Enhanced Context**: Building a specialized "knowledge base" for the database, which can include:
    * Detailed natural language descriptions of tables and fields.
    * Synonyms and business terminology mapping (e.g., "expense" to `cost` field).
    * Complex query examples (with `JOIN`, `GROUP BY`, subqueries).
    The system retrieves the most relevant knowledge and combines it with the user's question into a rich prompt, reducing hallucination risk.
4.  **Error Correction and Reflection**: After generating SQL, if the database returns an error, the system feeds the error message back to the LLM for **"reflection"** and correction of the SQL statement before retrying.

---

# Section 4: Query Rewriting and Distribution
This stage involves "preprocessing" the user's query before retrieval to overcome complexity, ambiguity, or poor alignment with document phrasing. The two key techniques are **Query Translation** and **Query Routing**.

## I. Query Translation
The goal is to bridge the **"semantic gap"** between the user's question and the stored information by rewriting, decomposing, or expanding the query.

### 1.1 Prompt Engineering
This uses a carefully designed **Prompt** to guide the LLM to rewrite the original query to be clearer, more specific, or better suited for retrieval. An advanced technique is prompting the LLM to directly construct a **structured instruction** (e.g., JSON format) that tells the application code how to handle complex intentions like sorting or finding "maximum/minimum" values.

### 1.2 Multi-query Decomposition
This technique addresses complex questions by breaking them down into multiple simpler, more specific **sub-questions**. Retrieval is performed for each sub-question, and all retrieved results are merged and de-duplicated to form a more comprehensive context for the final answer generation by the LLM.

### 1.3 Step-Back Prompting
This technique enhances LLM reasoning for highly specific questions.
* **Abstraction**: The LLM is first guided to generate a higher-level, more generalized **"Step-back Question"** from the original concrete question, aiming to find the universal principle or core concept.
* **Reasoning**: The system obtains the answer to the "Step-back Question" (general principle) and uses it as additional context to answer the original question.

## II. Query Routing
**Query Routing** involves intelligently distributing the question to the most appropriate data source or retriever based on the nature of the question.

### 2.1 LLM-based Router
The router uses an **LLM to perform intent recognition**. It manages a set of data sources, each with a textual description of its function, and the LLM selects the best source by matching the user's query with the descriptions.

### 2.2 Embedding-based Router
This method relies on **vector similarity**. A textual description of each data source and the user's query are both converted into vector embeddings, and the router selects the source whose vector is most similar to the query vector.

### 2.3 LlamaIndex Extension
LlamaIndex offers query routing by packaging different data sources or strategies as **"Tools"**. A **"Router"** dynamically selects the most suitable tool(s) for the user query, primarily using **LLM-based intent recognition**.

---

# Section 5: Advanced Retrieval Techniques
Advanced retrieval techniques are introduced to address limitations in basic RAG, such as semantic understanding deviations and the issue of the most relevant document not always being at the top.

## I. Re-ranking
Re-ranking aims to improve the quality of the initial set of retrieved documents.

### 1.1 RRF (Reciprocal Rank Fusion)
* A simple and effective **zero-shot** re-ranking method that relies purely on a document's **rank** in the result lists of multiple different retrievers.
* It ignores the original similarity scores, which may lead to loss of some useful information.

### 1.2 RankLLM / LLM-based Reranker
This method directly uses the **Large Language Model** itself to judge which context is most relevant, since the LLM is ultimately responsible for generating the answer. It uses a designed **prompt** containing the query and candidate document summaries, asking the LLM to output a sorted list based on relevance.

### 1.3 Cross-Encoder Reranking
* Provides excellent re-ranking accuracy.
* **Mechanism**: The Query and each candidate Document are **concatenated** into a single input (e.g., `[CLS] query [SEP] document [SEP]`) and fed into a Transformer model.
* The model outputs a single score representing the **relevance** of the pair.
* **Trade-off**: High precision but high latency because it requires N independent model inferences for N documents.

### 1.4 ColBERT Reranking
ColBERT (Contextualized Late Interaction over BERT) balances the high accuracy of Cross-Encoders and the high efficiency of Bi-Encoders using a **"Late Interaction"** mechanism.
* **Independent Encoding**: Generates context-aware embeddings for every Token in the Query and Document separately (document vectors can be pre-calculated).
* **Late Interaction**: At query time, it calculates the **maximum similarity (MaxSim)** between each Query Token vector and all Document Token vectors.
* **Score Aggregation**: All MaxSim scores for the Query Tokens are summed to get the final relevance score.

### 1.5 Comparison of Re-ranking Methods

| Feature | RRF | RankLLM | Cross-Encoder | ColBERT |
| :--- | :--- | :--- | :--- | :--- |
| **Core Mechanism** | Fusion of multiple rankings | LLM inference, generates a sorted list | Jointly encodes Query and Document | Independent encoding, Late Interaction |
| **Computational Cost** | Low (Simple math calculation) | Medium (API cost and latency) | High (N times model inference) | Medium (Vector dot product calculation) |
| **Interaction Granularity** | None (Rank only) | Concept/Semantic level | Sentence level (Query-Doc Pair) | Token level |
| **Applicable Scenarios** | Multi-way recall results fusion | High-value semantic understanding scenarios | Top-K fine-grained ranking | Top-K re-ranking |

## II. Compression
Compression techniques aim to address the issue of retrieved document chunks containing irrelevant **"noise"** text, which increases cost and can degrade answer quality.
* **Goal**: To **"compress"** and **"refine"** the retrieved content, keeping only the information most directly relevant to the user's query.
* **Methods**:
    1.  **Content Extraction**: Extracting only the relevant sentences or paragraphs from a document.
    2.  **Document Filtering**: Completely discarding entire documents that are deemed irrelevant after a more refined judgment.
* The general concept of a **Contextual Compression Retriever** involves a wrapper around a basic retriever that uses a specified **Document Compressor** to process the documents before returning them.

## III. Multi-hop Retrieval and Reasoning
This approach is needed when a question requires combining facts from **multiple, non-contiguous pieces of information** in the knowledge base, where a single direct search will fail. It involves a sequential process: **Retrieve** the first piece of information, use that information to **formulate a new query**, **retrieve** the second piece, and so on, until all necessary facts are gathered to construct the final answer.

## IV. Multi-modal RAG and RAG-Fusion
* **Multi-modal RAG**: Extends the RAG framework to handle various data types beyond text, such as images, audio, and video, by converting non-textual data into unified vector representations.
* **RAG-Fusion**: A pipeline that combines multiple search strategies (e.g., query rewriting, multi-query decomposition) to generate a richer set of retrieved documents before re-ranking and generating the final answer.

## V. Critic RAG (C-RAG)
Critic RAG is an advanced framework that introduces a **self-correction** and **knowledge boundary awareness** mechanism to the RAG pipeline.
* **Core Idea**: It incorporates a **Critic** module (usually an LLM) that evaluates the retrieval results.
* **Workflow**:
    1.  **Knowledge Assessment**: The Critic evaluates the retrieved context's quality against the user's question, resulting in one of four states: **Correct**, **Incorrect**, **Ambiguous**, or **Irrelevant**.
    2.  **State-based Action**:
        * If the context is assessed as **"Correct"**, it proceeds to **Refinement** (filtering out noise) and then **Answer Generation**.
        * If the context is assessed as **"Incorrect"** or **"Ambiguous"**, it triggers **Knowledge Searching** (Web Search) to actively seek external help.
* **Benefit**: Greatly enhances system robustness by adding a **"fact-checking"** layer, reducing hallucination, and improving accuracy.
